import pandas as pd
from sklearn import tree

# загружаем файл
X_data=pd.read_csv('train.csv')
X_test_data=pd.read_csv('test.csv')
print(X_test_data.shape)
# (418, 11)

print(X_data.shape)
# [891 rows x 12 columns]
# проверяем количество пропущенных значений
titanic_sum=X_data.isnull().sum()
print(titanic_sum)
'''
PassengerId      0
Survived         0
Pclass           0
Name             0
Sex              0
Age            177
SibSp            0
Parch            0
Ticket           0
Fare             0
Cabin          687
Embarked         2
dtype: int64

'''
# у-это то что мы будем вычислять
y=X_data.Survived
features=["Pclass", "Sex", "SibSp", "Parch"]
# создаем файл с выбранными данными для работы с ними
X=X_data[features]
print(X)
# к файлу с данными применяем функцию get_dummies()-для того что конвертировать
# string в числовые значения(0,1),используя метод onehotincoding
X=pd.get_dummies(X)
print(X)

# создаем дерево решений

# теперь все тоже самое осуществим и с тестовыми данными
X_test=pd.get_dummies(X_test_data)


clf=tree.DecisionTreeClassifier(criterion='entropy',random_state=1)
# обучаем дерево решений
clf.fit(X,y)
# создаем тестовые файлы для проверки работоспособности нашей машины
from sklearn.model_selection import train_test_split
train_X, valid_X, train_y, valid_y=train_test_split(X,y, test_size=0.33, random_state=42)
print(train_X.shape)
# (596, 10)
print(valid_X.shape)
# (295, 10)
# создадим дерево решений для test
clf=tree.DecisionTreeClassifier(criterion='entropy', random_state=1,
                                max_depth=5)
clf.fit(train_X,train_y)
predictions=clf.predict(valid_X)
print(predictions)

output=pd.DataFrame(
    {
        'PassengerId':X_test.PassengerId,
        'Survived':predictions
    }
)

prin(output)
